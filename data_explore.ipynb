{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Telemarketing Data\n",
    " \n",
    "Please see <code>db.py</code> and <code>processing.py</code> for the implentation of database and processing related utilities used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db import Database\n",
    "import processing as proc\n",
    "from pathlib import Path\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn\n",
    "import pickle\n",
    "\n",
    "logger = proc.setupLog()\n",
    "figureCount = 0\n",
    "records = Database('bank_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Taking a look\n",
    "We begin by checking the frequency of the target, <code>y</code>, which denotes whether or not a purchase was made.\n",
    "We can see that our data is imbalaced, with about nine examples with <code>y=0</code> to each one <code>y=1</code>.\n",
    "\n",
    "There are two seperate datasets here, both have the same target, with one having a few extra features, they are primarily economic indicators at the time of contact.  The datasets were taken from UCI's machine learning repository linked [here](http://archive.ics.uci.edu/ml/datasets/Bank+Marketing).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the abbreviated dataset a purchase occured 11.70% of the time.\n",
      "In the full dataset a purchase occured 11.27% of the time.\n",
      "In the combined dataset a purchase occured 11.49% of the time.\n"
     ]
    }
   ],
   "source": [
    "numPeople = records.getSize('people')\n",
    "numPeopleA = records.getSize('people_additional')\n",
    "results = records.getEntriesByField('people',0,numPeople,'y')\n",
    "resultsA = records.getEntriesByField('people_additional',0,numPeopleA,'y')\n",
    "##tally up number of Y/N s\n",
    "peopleBuy = 0\n",
    "for char in results:\n",
    "    if char == 'Y': peopleBuy+=1\n",
    "peopleBuyA = 0\n",
    "for char in resultsA:\n",
    "    if char == 'Y': peopleBuyA+=1\n",
    "\n",
    "buys = (peopleBuy,peopleBuyA,peopleBuy+peopleBuyA)\n",
    "notBuys = (numPeople-buys[0],numPeopleA-buys[1],numPeople+numPeopleA-buys[2])\n",
    "\n",
    "##Plot\n",
    "ind = [ i for i in range(len(buys)) ]\n",
    "width = .4\n",
    "plt.figure(figureCount)\n",
    "figureCount+=1\n",
    "plt.bar(ind,buys,width,label= 'Purchase')\n",
    "plt.bar([item+width for item in ind],notBuys,width,label='No Purchase')\n",
    "plt.title('Purchase frequency by dataset')\n",
    "plt.xticks([item+width/2 for item in ind],('abbreviated dataset','full dataset', 'overall'))\n",
    "plt.show()\n",
    "\n",
    "print(\"In the abbreviated dataset a purchase occured {:.2f}% of the time.\".format(\n",
    "     ( float(buys[0])/float(numPeople) )*100 ) )\n",
    "print(\"In the full dataset a purchase occured {:.2f}% of the time.\".format(\n",
    "     ( float(buys[1])/float(numPeopleA) )*100 ) )\n",
    "print(\"In the combined dataset a purchase occured {:.2f}% of the time.\".format(\n",
    "     ( float(buys[2])/float(numPeople+numPeopleA) )*100 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting our dataset ready for processing\n",
    "The dataset has been extracted from csv, translated for efficient storage (strings mapped to characters etc.) and loaded into a MySQL database.\n",
    "Below is a few of the fetched SQL entries and human-readable dataframes lines.  For completeness, a single final encoding of the ready-for-work numpy vector is also displayed.\n",
    "*Note: We remove the <code>duration</code> field as reccomended in the dataset posting.  See the link above for more information.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 of the sql entries:\n",
      "(58, 'M', 'M', 'T', 'N', 2143, 'Y', 'N', '?', 5, 5, 261, 1, -1, 0, '?', 'N')\n",
      "(44, 'T', 'S', 'S', 'N', 29, 'Y', 'N', '?', 5, 5, 151, 1, -1, 0, '?', 'N')\n",
      "(33, 'E', 'M', 'S', 'N', 2, 'Y', 'Y', '?', 5, 5, 76, 1, -1, 0, '?', 'N')\n",
      "(47, 'B', 'M', '?', 'N', 1506, 'Y', 'N', '?', 5, 5, 92, 1, -1, 0, '?', 'N')\n",
      "(33, '?', 'S', '?', 'N', 1, 'N', 'N', '?', 5, 5, 198, 1, -1, 0, '?', 'N')\n",
      "\n",
      "\n",
      "Head of the dataframe:\n",
      "   age           job  marital  education isDefault  balance housing loan  \\\n",
      "0   58    management  married   tertiary        no     2143     yes   no   \n",
      "1   44    technician   single  secondary        no       29     yes   no   \n",
      "2   33  entrepreneur  married  secondary        no        2     yes  yes   \n",
      "3   47   blue-collar  married    unknown        no     1506     yes   no   \n",
      "4   33       unknown   single    unknown        no        1      no   no   \n",
      "\n",
      "   contact  day  month  duration  campaign  pdays  previous poutcome   y  \n",
      "0  unknown    5      5       261         1     -1         0  unknown  no  \n",
      "1  unknown    5      5       151         1     -1         0  unknown  no  \n",
      "2  unknown    5      5        76         1     -1         0  unknown  no  \n",
      "3  unknown    5      5        92         1     -1         0  unknown  no  \n",
      "4  unknown    5      5       198         1     -1         0  unknown  no  \n",
      "\n",
      "\n",
      "Finally, here is a single vector ready for processing:\n",
      "[5.800e+01 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      " 1.000e+00 0.000e+00 2.143e+03 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 5.000e+00 5.000e+00\n",
      " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      " 0.000e+00]\n"
     ]
    }
   ],
   "source": [
    "#get data dictionaries for processing\n",
    "peopleMap, peopleAdditionalMap = records.getMaps()\n",
    "\n",
    "#sql entries\n",
    "line = records.getEntries('people',0,numPeople)\n",
    "\n",
    "print('First 5 of the sql entries:')\n",
    "for i in range(5):\n",
    "    print(line[i])\n",
    "    \n",
    "#translate to human-readable dataframe (similar to original csv)\n",
    "df = proc.getDataFrame(line,peopleMap)\n",
    "print('\\n\\nHead of the dataframe:')\n",
    "print(df.head())\n",
    "\n",
    "#see validate for more details, basically ensures all categories accounted for properly\n",
    "proc.validate(df,peopleMap)\n",
    "vec = proc.getOneHot(df,peopleMap,False)\n",
    "\n",
    "#duration is not known until outcome is known\n",
    "#see dataset documentation for more info\n",
    "vec = vec.drop('duration',axis=1)\n",
    "dataOneHot = vec.to_numpy(dtype=np.float64)\n",
    "\n",
    "print('\\n\\nFinally, here is a single vector ready for processing:')\n",
    "print(dataOneHot[0])\n",
    "\n",
    "\n",
    "#do the same as above for tha additional dataset\n",
    "lineA = records.getEntries('people_additional',0,numPeopleA)\n",
    "dfa = proc.getDataFrame(lineA,peopleAdditionalMap)\n",
    "proc.validate(dfa,peopleAdditionalMap)\n",
    "vecA = proc.getOneHot(dfa,peopleAdditionalMap,False)\n",
    "vecA = vecA.drop('duration',axis=1)\n",
    "dataOneHotA = vecA.to_numpy(dtype=np.float64)\n",
    "\n",
    "#get column labels\n",
    "colTitles = vec.columns\n",
    "colTitlesA = vecA.columns\n",
    "\n",
    "#rand generator\n",
    "randomGenr = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting organized\n",
    "\n",
    "Here the datasets and models are generated and setup.<br>\n",
    "The data is processed in its raw form, and also after scaling it with \n",
    "\n",
    "[<code>sklearn.preprocessing.StandardScaler</code>](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)<br>\n",
    "\n",
    "The followind models are then run on both sets:<br>\n",
    "\n",
    "[<code>sklearn.neighbors.KNeighborsClassifier</code>](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)<br>\n",
    "[<code>sklearn.ensemble.RandomForestClassifier</code>](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)<br>\n",
    "[<code>sklearn.neural_network.MLPClassifier</code>](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrees = 1000 #rafo trees\n",
    "maxIter = 1000 #for MLP\n",
    "k = 9 #neighbors for knn\n",
    "\n",
    "\n",
    "#initialize models\n",
    "rafo = RandomForestClassifier(n_estimators=numTrees,class_weight='balanced')\n",
    "rafoA = RandomForestClassifier(n_estimators=numTrees,class_weight='balanced')\n",
    "mlp = MLPClassifier(max_iter=maxIter)\n",
    "mlpA = MLPClassifier(max_iter=maxIter)\n",
    "knn = KNeighborsClassifier(k)\n",
    "knnA = KNeighborsClassifier(k)\n",
    "\n",
    "models = { 'RAFO': (rafo,rafoA),#random forest\n",
    "          'MLP' : (mlp,mlpA),   #multi level perceptron\n",
    "          'KNN': (knn,knnA)     #k nearest neighbor\n",
    "         }\n",
    "\n",
    "#initialize models\n",
    "rafoS = RandomForestClassifier(n_estimators=numTrees,class_weight='balanced')\n",
    "rafoAS = RandomForestClassifier(n_estimators=numTrees,class_weight='balanced')\n",
    "mlpS = MLPClassifier(max_iter=maxIter)\n",
    "mlpAS = MLPClassifier(max_iter=maxIter)\n",
    "knnS = KNeighborsClassifier(k)\n",
    "knnAS = KNeighborsClassifier(k)\n",
    "\n",
    "modelsScaled = { 'RAFO': (rafoS,rafoAS),#random forest\n",
    "                 'MLP' : (mlpS,mlpAS),  #multi level perceptron\n",
    "                 'KNN' : (knnS,knnAS)   #k nearest neighbor\n",
    "                } \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model\n",
    "The models are run and relavent information stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial: 10\n"
     ]
    }
   ],
   "source": [
    "accumulator = []\n",
    "results = {key:dict({ke:None for ke in ['probabilities','predictions']}) for key in models.keys()}\n",
    "resultsScaled = {key:dict({ke:None for ke in ['probabilities','predictions']}) for key in modelsScaled.keys()}\n",
    "totalIterations = 10\n",
    "testLabels = [None for _ in range(totalIterations)]\n",
    "testLabelsA = [None for _ in range(totalIterations)]\n",
    "for i in range(totalIterations):\n",
    "    randomGenr.shuffle(dataOneHot)\n",
    "    randomGenr.shuffle(dataOneHotA)\n",
    "    #extract features, set float\n",
    "    features = dataOneHot[:,:-1].astype(np.float64)\n",
    "    featuresA = dataOneHotA[:,:-1].astype(np.float64)\n",
    "    #extract labels\n",
    "    labels = dataOneHot[:,-1]\n",
    "    labelsA = dataOneHotA[:,-1]    \n",
    "    #get ~20% of indices as random choice WITHOUT replacement for test\n",
    "    testIndices = randomGenr.choice(len(labels),size=int(len(labels)*.2),replace=False)\n",
    "    testIndicesA = randomGenr.choice(len(labelsA),size=int(len(labelsA)*.2),replace=False)\n",
    "    #take test items\n",
    "    \n",
    "    testLabels[i] = np.take(labels,testIndices)\n",
    "    testFeatures = np.take(features,testIndices,axis=0)\n",
    "    testLabelsA[i] = np.take(labelsA,testIndicesA)\n",
    "    testFeaturesA = np.take(featuresA,testIndicesA,axis=0)\n",
    "    \n",
    "    #remove test items\n",
    "    trainLabels = np.delete(labels,testIndices)\n",
    "    trainFeatures = np.delete(features,testIndices,axis=0)\n",
    "    trainLabelsA = np.delete(labelsA,testIndicesA)\n",
    "    trainFeaturesA = np.delete(featuresA,testIndicesA,axis=0)\n",
    "    \n",
    "    #create scaled features\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    trainFeaturesS = scaler.fit_transform(trainFeatures)\n",
    "    testFeaturesS = scaler.fit_transform(testFeatures)\n",
    "    trainFeaturesAS = scaler.fit_transform(trainFeaturesA)\n",
    "    testFeaturesAS = scaler.fit_transform(testFeaturesA)\n",
    "    \n",
    "    print('trial: {}'.format(10))\n",
    "    for name,model,modelS in zip(models.keys(),models.values(),modelsScaled.values()):\n",
    "        model[0].fit(trainFeatures,trainLabels)\n",
    "        model[1].fit(trainFeaturesA,trainLabelsA)\n",
    "        modelS[0].fit(trainFeaturesS,trainLabels)\n",
    "        modelS[1].fit(trainFeaturesAS,trainLabelsA)\n",
    "    \n",
    "        #get predictions\n",
    "        preds = model[0].predict(testFeatures)\n",
    "        predsA = model[1].predict(testFeaturesA)\n",
    "        predsS = modelS[0].predict(testFeaturesS)\n",
    "        predsAS = modelS[1].predict(testFeaturesAS)\n",
    "        results[name]['predictions'] = [preds,predsA]\n",
    "        resultsScaled[name]['predictions'] = [predsS,predsAS]\n",
    "    \n",
    "        #get P(y=1) for each prediction\n",
    "        probs = [pair[1] for pair in model[0].predict_proba(testFeatures)]\n",
    "        probsA = [pair[1] for pair in model[1].predict_proba(testFeaturesA)]\n",
    "        probsS = [pair[1] for pair in modelS[0].predict_proba(testFeaturesS)]\n",
    "        probsAS = [pair[1] for pair in modelS[1].predict_proba(testFeaturesAS)]\n",
    "        results[name]['probabilities'] = (probs,probsA)\n",
    "        resultsScaled[name]['probabilities'] = (probsS,probsAS)\n",
    "        accumulator.append([results,resultsScaled])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tallying up results\n",
    "Finally, we tally the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tallys = {key:np.empty((2,2,2,0),dtype='float64').tolist() for key in models.keys()}\n",
    "tallysS = {key:np.empty((2,2,2,0),dtype='float64').tolist() for key in modelsScaled.keys()}\n",
    "for entry,tLabels,tLabelsA in zip(accumulator,testLabels,testLabelsA):\n",
    "    for key in results.keys():\n",
    "        results = entry[0]\n",
    "        resultsScaled = entry[1]\n",
    "        #SETUP FOR THIS:\n",
    "        #tallys[modelName][0=firstDataSet,1=AdditionalDataSet][trueLabel][predictedLabel] = [list of P(y_{model}=1)]\n",
    "        j=0\n",
    "        for labels,predictions,predictionsS in zip([tLabels,tLabelsA],results[key]['predictions'],resultsScaled[key]['predictions']):\n",
    "            i=0\n",
    "            for test,pred,predS in zip(labels,predictions,predictionsS):\n",
    "                    tallys[key][j][int(test)][int(predS)].append(results[key]['probabilities'][j][i])\n",
    "                    tallysS[key][j][int(test)][int(predS)].append(resultsScaled[key]['probabilities'][j][i])\n",
    "                    i+=1\n",
    "            j+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking error rates\n",
    "\n",
    " \n",
    "We can see that the best performing model on the abbreviated feature data is Random Forest, while K Nearest Neighbors performed best on the full featured dataset.  Scaling the data is not helpful in either case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot raw error rates of each model\n",
    "ind = [ i for i in range(len(models)) ]\n",
    "width = .6\n",
    "plt.figure(figureCount,figsize=(40,40))\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "titles = ['abbreviated features','full features'],['abbreviated features, scaled','full features, scaled']\n",
    "tally = [tallys,tallysS]\n",
    "totals = [sum([len(testLabels[i]) for i in range(totalIterations)]),sum([len(testLabelsA[i]) for i in range(totalIterations)])]\n",
    "figureCount+=1\n",
    "for i in range(2):\n",
    "    for j in range (2):\n",
    "        wrong = [(len(tally[i][key][j][1][0])+len(tally[i][key][j][0][1]))/totals[i] for key in tallys.keys()]\n",
    "        print(wrong)\n",
    "        #set bottom for each graph\n",
    "        bottom = min(wrong)-.005\n",
    "        ax[i,j].bar([item+width for item in ind],[item-bottom for item in wrong],width,label='Incorrect Predictions',color='red',bottom=bottom)\n",
    "        ax[i,j].set_xticks([item+width/2 for item in ind])\n",
    "        ax[i,j].set_xticklabels(tallys.keys())\n",
    "        ax[i,j].set_title(titles[i][j])\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Error rates',verticalalignment='baseline')\n",
    "plt.show()\n",
    "print('RAFO is Random Forest\\nMLP is Multi Level Perceptron\\nKNN is K Nearest Neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While ~90% success rate is not terrible on a balanced dataset.  We know that our dataset is biased, and so this prediction success rate is about equal to the error rate obtained by guessing randomly according to the distribution of classes.  Moving forward, we'll only work with the non-scaled data since the scaling wasn't helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slightly modifying our metric\n",
    "Lets define some things, first.<br>\n",
    "Define a test set:\n",
    "$$   D^{test} = \\{ ( x_{i} ,y_{i}) \\mid 0 \\leq i \\lt N \\}  $$\n",
    "    Where there are N examples in a dataset with feature vector x, label y\n",
    "\n",
    "Define the previously learned *estimator* $ f_{\\theta}(\\cdot) $, parameterized by $\\theta$:\n",
    "$$ f_{\\theta}(x_{i}) = \\hat{y} \\approx y_{i} $$\n",
    "and let\n",
    "$$ P_{\\theta}(y_{i}=1\\mid x_{i}) = p_{i} $$\n",
    "for a given test example $( x_{i} ,y_{i})$<br><br>\n",
    "and finally we define the *indicator* $ I_{i} $: <br><br>\n",
    "$$ I_{i} = 1 \\iff   y_{i}=1 \\:  $$<br>\n",
    "$$ else \\: \\: \\: I_{i} = 0 $$\n",
    "\n",
    "and now define a sorted list:\n",
    "$$ \\mathcal{S} = \\{  s^{j} = (s_{p}^{j},s_{I}^{j}) = ( p_{i} ,I_{i}) \\mid 0 \\leq i,j \\lt N \\: , \\: a \\gt b \\implies s_{p}^{a} \\gt s_{p}^{b} \\} $$ \n",
    "\n",
    "In English: $\\mathcal{S}$ is a list of indicator-probabilty pairs (like a priority queue) sorted in increasing order by probabilities.<br><br>\n",
    "Now,\n",
    "$$ \\mathcal{S}_{t} = \\{ s^{j} \\mid 0 \\leq j \\lt floor(\\frac{t}{100})(N) \\: , \\: 1 \\leq t \\leq 100 \\} $$\n",
    "\n",
    "In English: $\\mathcal{S}_{t}$ is the first \"$t \\%$\"  of $\\mathcal{S}$.\n",
    "\n",
    "Lastly we will define,\n",
    "$$ S(t)  =   \\frac{\\sum_{j \\leq \\mid \\mathcal{S}_{t}\\mid} s_{p}^{j}}{\\sum_{j \\leq \\mid \\mathcal{S}\\mid} s_{p}^{j}} $$\n",
    "\n",
    "In English: $ S(t) $ is the percentage of true positives inside $\\mathcal{S}_{t}$.\n",
    "$ S(t) $ essentially describes the percentage of <code>y=1</code> labeled test samples in the top $t\\%$ of the model's probability ranking.\n",
    "\n",
    "\n",
    "We'll now make some plots of $S(t)$ for each model dataset pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tallys[modelName][0=firstDataSet,1=AdditionalDataSet][trueLabel][predictedLabel] = [list of P(y_{model}=1)]\n",
    "#array to study\n",
    "s = {key:[None,None] for key in tallys.keys()}\n",
    "points ={key: [None,None] for key in tallys.keys()}\n",
    "totals = [sum([sum(testLabels[i]) for i in range(totalIterations)]),sum([len(testLabelsA[i]) for i in range(totalIterations)])]\n",
    "##TODO:: SOMEHOW THE NUMBER OF TRUE POSITIVES IS DIFFERENT PER MODEL, THIS SHOULD NOT BE THE CASE\n",
    "for key in tallys.keys():\n",
    "    for i in range(2):\n",
    "        #take true positives out, most important\n",
    "        positives = np.append([tp for tp in tallys[key][i][1][1]],[fn for fn in tallys[key][i][1][0]])\n",
    "        #take count of totals while convenient\n",
    "        #take false positives, true negatives OUT\n",
    "        others = np.append([tn for tn in tallys[key][i][0][0]],[fp for fp in tallys[key][i][0][1]])\n",
    "        #add true positives to S\n",
    "        positives = [ (1,prob) for prob in positives ]\n",
    "        others = [ (0,prob) for prob in others ]\n",
    "        #get final array\n",
    "        s[key][i] = sorted(np.concatenate((others,positives)),key=lambda key_value: key_value[1],reverse=True)\n",
    "        points[key][i] = [(t,proc.S(s[key][i],t,totals[i])) for t in range(1,101)]\n",
    "        #generate points for graphing\n",
    "for i in range(2):\n",
    "    #genrate set of 100 points, t S(t) pairs for each model/dataset pair\n",
    "    plt.figure(figureCount)\n",
    "    figureCount+=1\n",
    "    fig, ax = plt.subplots()\n",
    "    for key,color in zip(points.keys(),['black','green','blue']):\n",
    "        ax.plot([point[0] for point in points[key][i]],[point[1] for point in points[key][i]],color,label=key)\n",
    "        ax.set(xlabel='t', ylabel='S(t)',title=('Full features' if i==1 else 'Abbreviated features'))\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    fig.savefig(\"S(t)_{}_{}.png\".format(key,i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Taking a look inside\n",
    "\n",
    "Finally, lets take a look at the factors involved in making these predictions for the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [testLabels[0],testLabelsA[0]]\n",
    "features = [testFeatures,testFeaturesA]\n",
    "cols = [colTitles,colTitlesA]\n",
    "maps = [peopleMap,peopleAdditionalMap]\n",
    "titles =['Feature importance for abbreviated features','Feature importance for full features']\n",
    "for i in range(2):\n",
    "    importances = permutation_importance(models['RAFO'][i],features[i],labels[i],n_repeats=10)['importances_mean']\n",
    "    result = proc.processImportance(maps[i],cols[i],importances)\n",
    "    proc.pie(result[0],result[1],result[2],titles[i],figureCount)\n",
    "    figureCount+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Discussion*\n",
    "\n",
    "These graphs provide us with a great deal of insight.  For the abbreviated dataset, the Random Forest is a clearly the best performer.  While the classification error rate between the Random Forest and the Multi-Level-Perceptron is quite small.  The Random Forest clearly outperforms MLP by a sizeable margin for its probability ranking. With respect to the full dataset the MLP barely outpaces Random Forest.  Regardless, a graph like this is useful in model selection in  determing which to use to rank items probabilistically.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you to the authors of [Moro et al., 2014](https://core.ac.uk/download/pdf/55631291.pdf) for the dataset and inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
